from transformers import AutoTokenizer
from collections import Counter

def count_unique_tokens(text, model_name='bert-base-uncased'):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokens = tokenizer.tokenize(text)
    token_counts = Counter(tokens)
    return token_counts.most_common(30)

text_file_path = 'D:/assignment 2/combined_txt.txt'
model_name = 'bert-base-uncased'

with open(text_file_path, 'r') as file:
    text = file.read()

top_30_tokens = count_unique_tokens(text)
print(top_30_tokens)